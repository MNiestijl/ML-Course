\documentclass [a4paper] {report}
\usepackage{amsmath,amssymb,amsthm, bbm, graphicx,listings,braket,subfig,titlesec,cleveref,lipsum,mcode,xcolor-patch, textcomp,float,booktabs,siunitx, listings}
\usepackage[authoryear]{natbib}
\usepackage[section]{placeins}
\usepackage[margin=2.2cm]{geometry}
\titleformat{\chapter}{\normalfont\huge}{\thechapter.}{20pt}{\huge \bf}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\begin{document}
	
	\begin{titlepage}
		\begin{center}
			
			\textsc{\LARGE IN4320 Machine Learning}\\[1.25cm]
			
			\rule{\linewidth}{0.5mm}\\[1.0cm]
			{\huge \bfseries Exercises: Reinforcement Learning }\\[0.6cm]
			\rule{\linewidth}{0.5mm}\\[1.5cm]
			
			\begin{minipage}{0.4\textwidth}
				\begin{flushleft} \large	
					\emph{Author:}\\
					\textsc{Milan Niestijl, 4311728}
				\end{flushleft}
			\end{minipage}
			
			\vfill
			{\large \today}
		\end{center}
	\end{titlepage}
	
	\section*{Exercise 1}
	\textbf{Claim}\\
	The return $R_{t} = \sum_{h=0}^{\infty}\gamma^{h}r_{t+h+1}$ is bounded for $0\leq \gamma<1$ and bounded rewards $-10\leq r_{t+h+1}\leq 10$ for all $h\in \mathbb{N}$.\\\\
	\textbf{Proof}\\
	Using the geometric series, we find:
	$$|R_{t}| \leq \sum_{h=0}^{\infty}|\gamma^{h}r_{t+h+1}| \leq \sum_{h=0}^{\infty}10 \gamma^{h} = {10 \over 1-\gamma} < \infty$$
	
	\section*{Exercise 2}
	The optimal policy for $\gamma=0.5$, the Q-function is shown after each iteration in the tables \ref{Qtable}.
	
	\begin{table}[H]
		\centering
		\caption{Q-value for different iterations.}
		\label{Qtable}
		\begin{tabular}{l|llllll}
			Action\textbackslash State    & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			Left 				& 0 & 0 & 0 & 0 & 0 & 0 \\ 
			Right 			& 0 & 0 & 0 & 0 & 0 & 0 \\ 
		\end{tabular}
		
		\begin{tabular}{l|llllll}
			Action\textbackslash State    & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			Left 				& 0 & 1 & 0 & 0 & 0 & 0 \\ 
			Right 			& 0 & 0 & 0 & 0 & 5 & 0 \\ 
		\end{tabular}
		
		\begin{tabular}{l|llllll}
			Action\textbackslash State    & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			Left 				& 0 & 1 & 0.5 & 0 & 0 & 0 \\ 
			Right 			& 0 & 0 & 0 & 2.5 & 5 & 0 \\ 
		\end{tabular}
		
		\begin{tabular}{l|llllll}
			Action\textbackslash State    & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			Left 				& 0 & 1 & 0.5 & 0.625 & 1.25 & 0 \\ 
			Right 			& 0 & 0.625 & 1.25 & 2.5 & 5 & 0 \\ 
		\end{tabular}
		
	\end{table}
	
	\noindent
	The resulting optimal policy is shown in table \ref{optPolicy}.
	
	\begin{table}[H]
		\centering
		\caption{Optimal policy for $\gamma=0.5$.}
		\label{optPolicy}
		\begin{tabular}{l|llllll}
			$s$    			& 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			$\pi(s)$			& Right & Left & Right & Right & Right & Right \\ 
		\end{tabular}
	\end{table}
	
	
	\section*{Exercise 3}
	The optimal value functions for $\gamma \in \{ 0,0.1, 0.9, 1 \}$ are shown in table \ref{optQs}.
	Note that $\gamma=1$ will work here because the reward function has a finite horizon.
	
	\begin{table}[H]
		\centering
		\caption{$Q*$ for various values of the discount factor $\gamma$.}
		\label{optQs}
		\textbf{$\gamma=0$:}
		\begin{tabular}{l|llllll}
			Action\textbackslash State    & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			Left 				& 0 & 1 & 0 & 0 & 0 & 0 \\ 
			Right 			& 0 & 0 & 0 & 0 & 5 & 0 \\ 
		\end{tabular}
		
		\textbf{$\gamma=0.1$:}
		\begin{tabular}{l|llllll}
			Action\textbackslash State    & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			Left 				& 0 & 1 & 0.1 & 0.01 & 0.005 & 0 \\ 
			Right 			& 0 & 0.01 & 0.05 & 0.5 & 5 & 0 \\ 
		\end{tabular}
		
		\textbf{$\gamma=0.9$:}
		\begin{tabular}{l|llllll}
			Action\textbackslash State    & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			Left 				& 0 & 1 & 3.2805 & 3.645 & 4.05 & 0 \\ 
			Right 			& 0 & 3.645 & 4.05 & 4.5 & 5 & 0 \\ 
		\end{tabular}
		
		\textbf{$\gamma=1$:}
		\begin{tabular}{l|llllll}
			Action\textbackslash State    & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
			Left 				& 0 & 1 & 5 & 5 & 5 & 5 \\ 
			Right 			& 0 & 5 & 5 & 5 & 5 & 0 \\ 
		\end{tabular}
		
	\end{table}

	\section*{Exercise 4}
	In figure ..., the 2-norm error of the learned Q-function is plotted versus the number of iterations for several values of the exploration rate $\epsilon$ and the learning rate $\alpha$. It can be seen that for larger values of $\epsilon$, the error decreases faster. This is explained by the fact that more often a random action is taken, which effectively allows the system to gain more new information and alter its values accordingly. Similarly, for larger values of $\alpha$, the learned Q-function on average updates with larger steps, which is reflected in the figure by the fact that the error makes larger 'jumps'. In this case, this causes the algorithm to converge faster.
	
	\section*{Exercise 5}
	In this case, q-iteration still converges as it uses the probabilities, although to different values as before (as expected). The iterations are deterministic. Q-learning however, no longer converges as the iterations are stochastic in nature and the learning rate is fixed. The values keep changing depending on whether the robot fails or not during that step. 

\end{document}