\documentclass [a4paper] {report}
\usepackage{amsmath,amssymb,amsthm, bbm, graphicx,listings,braket,subfig,titlesec,cleveref,lipsum,mcode,xcolor-patch, textcomp,float,booktabs,siunitx, listings}
\usepackage[authoryear]{natbib}
\usepackage[section]{placeins}
\usepackage[margin=2.2cm]{geometry}
\titleformat{\chapter}{\normalfont\huge}{\thechapter.}{20pt}{\huge \bf}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\begin{document}
	
	\begin{titlepage}
		\begin{center}
			
			\textsc{\LARGE IN4320 Machine Learning}\\[1.25cm]
			
			\rule{\linewidth}{0.5mm}\\[1.0cm]
			{\huge \bfseries Exercises: Multiple Instance Learning }\\[0.6cm]
			\rule{\linewidth}{0.5mm}\\[1.5cm]
			
			\begin{minipage}{0.4\textwidth}
				\begin{flushleft} \large	
					\emph{Author:}\\
					\textsc{Milan Niestijl, 4311728}
				\end{flushleft}
			\end{minipage}
			
			\vfill
			{\large \today}
		\end{center}
	\end{titlepage}
	
	\section*{1. Naive MIL classifier}
	We first implement a naive MIL classifier. To reduce computational costs, the images are first downscaled with a scaling factor of 0.2. The images are next segmented using the Mean Shift algorithm using a width parameter of 30. To verify that the apples are correctly separated from the background, the obtained segments are plotted for a couple of images in figure \textbf{TODO}.\\\\
	- 120 bags, 786/120=5.55 instances per bag and 3 features per instance. Check ff of deze shit klopt!\\
	A scatterplot of the instances can be seen in figure \textbf{TODO}.\\\\
	
	1.f:\\
	
	Number of apples misclassified as banana: 10\\
	Number bananas misclassified as apple: 14\\
	These error estimates are not trustworthy as one could simply classify all bags as an apple, resulting in zero apples being misclassified. Conversely, one could classify all bags as bananas causing zero bananas to be misclassified. A more trustworthy error estimate is the average number of misclassified bags:
	$$ \text{error} = {\#\text{misclassified bags}\over \#\text{bags}} $$
	Which, in our case results in an error of 0.2.\\
	
	1.g: \\
	
	Firstly, instances corresponding to background should not be classified as either a banana or an apple. Instead, it would be better to perform MIL classification twice; once to check whether or not the image contains an apple, and once to check whether or not is has a banana. In this manner, the background can be recognized as neither an apple nor a banana, instead of being forced to be classified as either a banana or an apple. If it is predicted that the image contains both an apple and a banana, you can either leave it at that or use some combining rule to choose either of the two, e.g., using some confidence measure. This would depend on whether or not it is assumed that the bag can contain only one of the two 'concepts'.\\
	
	Secondly, in checking whether or not the image contains an apple (or a banana), the combiner should return positive if at least one of the instances in the bag is predicted positive, instead of using majority voting. This makes intuitively more sense since only one of the segments in the image has to be an apple for the image to contain an apple.
	
	\section*{2. MILES}
	2.a:\\
	
	size of m(Bi) = number if instances (=786)\\
	met sigma=20, c=10 heb ik een error van 0! 
	
	\bibliographystyle{authordate1}
	\begin{bibliography}{ref}
		
	\end{bibliography}

\end{document}